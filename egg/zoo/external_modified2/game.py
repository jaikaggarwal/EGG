# adapted from https://github.com/facebookresearch/EGG

from __future__ import print_function

import argparse
import contextlib
import sys
import numpy as np

import torch
import torch.nn.functional as F
import torch.utils.data
from torch.utils.data import DataLoader
from torch.distributions import Categorical

import egg.core as core
from egg.core.language_analysis import TopographicSimilarity

from archs import Receiver, Sender, RnnSenderGS, SenderReceiverRnnGS, BiReceiver, BiRnnReceiverSTGS, SenderReceiverBiRnnSTGS
from exp_concrete import ExpRelaxedCategorical
from features import CSVDataset


def get_params():
    parser = argparse.ArgumentParser()
    parser.add_argument('--train_data', type=str, default=None,
                        help='Path to the train data')
    parser.add_argument('--validation_data', type=str, default=None,
                        help='Path to the validation data')
    parser.add_argument('--dump_data', type=str, default=None,
                        help='Path to the data for which to produce output information')
    parser.add_argument('--dump_output', type=str, default=None,
                        help='Path for dumping output information')

    parser.add_argument('--batches_per_epoch', type=int, default=1000,
                        help='Number of batches per epoch (default: 1000)')

    parser.add_argument('--sender_hidden', type=int, default=10,
                        help='Size of the hidden layer of Sender (default: 10)')
    parser.add_argument('--receiver_hidden', type=int, default=10,
                        help='Size of the hidden layer of Receiver (default: 10)')

    parser.add_argument('--sender_embedding', type=int, default=10,
                        help='Dimensionality of the embedding hidden layer for Sender (default: 10)')
    parser.add_argument('--receiver_embedding', type=int, default=10,
                        help='Dimensionality of the embedding hidden layer for Receiver (default: 10)')

    parser.add_argument('--sender_cell', type=str, default='rnn',
                        help='Type of the cell used for Sender {rnn, gru, lstm} (default: rnn)')
    parser.add_argument('--receiver_cell', type=str, default='rnn',
                        help='Type of the cell used for Receiver {rnn, gru, lstm} (default: rnn)')
    parser.add_argument('--sender_layers', type=int, default=1,
                        help="Number of layers in Sender's RNN (default: 1)")
    parser.add_argument('--receiver_layers', type=int, default=1,
                        help="Number of layers in Receiver's RNN (default: 1)")
    parser.add_argument("--bidirectional", help="Use a bidirectional receiver; only when ST", action="store_true")

    parser.add_argument('--sender_lr', type=float, default=1e-1,
                        help="Learning rate for Sender's parameters (default: 1e-1)")
    parser.add_argument('--receiver_lr', type=float, default=1e-1,
                        help="Learning rate for Receiver's parameters (default: 1e-1)")
    parser.add_argument('--temperature', type=float, default=1.0,
                        help="GS temperature for the sender (default: 1.0)")
    parser.add_argument("--st", action="store_true",
                        help="Use st GS")
    parser.add_argument('--alpha', type=float, default=1.0,
                        help="Cost for length-based prior (default: 1.0)")
    parser.add_argument('--output_strings', type=str, default='output_strings.txt',
                        help="Output file for strings generated by speaker")

    args = core.init(parser)
    return args

def differentiable_loss(n_features, alphabet_size, max_len, alpha):
    '''
    temperature = torch.tensor([1.2])
    probs_0 = torch.tensor([1./alphabet_size for _ in range(alphabet_size)])
    probs_i = torch.tensor([0.5] + [0.5/alphabet_size for _ in range(alphabet_size-1)])

    prior_0 = ExpRelaxedCategorical(temperature=temperature, probs=probs_0)
    prior_i = ExpRelaxedCategorical(temperature=temperature, probs=probs_i)

    def loss(sender_input, length, message, logits, receiver_input, receiver_output, labels):
        reconstruction_loss = F.binary_cross_entropy(receiver_output, sender_input.view(-1, n_features), reduction='none').mean(dim=-1)
        kl_loss = 0.
        log_message = torch.log(message)
        for i in range(min(length, max_len)):
            message_i = log_message[:, i, ...]
            logits_i = logits[:, i, ...]
            q_i = ExpRelaxedCategorical(temperature=temperature,logits=logits_i)
            if i == 0:
                kl_loss -= (prior_0.log_prob(message_i) - q_i.log_prob(message_i))
            else:
                kl_loss -= (prior_i.log_prob(message_i) - q_i.log_prob(message_i))
        return reconstruction_loss + kl_loss, {'reconstruction_loss': reconstruction_loss}
    '''

    '''
    probs_0 = torch.tensor([1e-20] + [(1-1e-20)/(alphabet_size-1) for _ in range(alphabet_size-1)])
    probs_i = torch.tensor([0.5] + [0.5/(alphabet_size-1) for _ in range(alphabet_size-1)])

    prior_0 = Categorical(probs=probs_0)
    prior_i = Categorical(probs=probs_i)

    def loss(sender_input, length, message, logits, receiver_input, receiver_output, labels):
        reconstruction_loss = F.binary_cross_entropy(receiver_output, sender_input.view(-1, n_features), reduction='none').mean(dim=-1)
        kl_loss = 0.
        for i in range(min(length, max_len)):
            message_i = message[:, i, ...].argmax(-1)
            logits_i = logits[:, i, ...]
            q_i_dist = Categorical(logits=logits_i)
            q_i = q_i_dist.log_prob(message_i)
            if i == 0:
                kl_loss -= (prior_0.log_prob(message_i) - q_i)
            else:
                kl_loss -= (prior_i.log_prob(message_i) - q_i)
        return reconstruction_loss + kl_loss, {'reconstruction_loss': reconstruction_loss}
    '''

    def loss(sender_input, length, message, logits, receiver_input, receiver_output, labels):
        reconstruction_loss = F.binary_cross_entropy(receiver_output, sender_input.view(-1, n_features), reduction='none').mean(dim=-1)
        return reconstruction_loss, {'reconstruction_loss': reconstruction_loss}

    return loss

def build_model(opts, train_loader, dump_loader):
    n_features = train_loader.dataset.get_n_features() if train_loader else dump_loader.dataset.get_n_features()
    receiver_outputs = n_features
    max_len = opts.max_len
    alpha = opts.alpha
    alphabet_size = opts.vocab_size

    sender = Sender(n_hidden=opts.sender_hidden, n_features=n_features)
    receiver = None
    if opts.bidirectional:
        receiver = BiReceiver(output_size=receiver_outputs, n_hidden=opts.receiver_hidden)
    else:
        receiver = Receiver(output_size=receiver_outputs, n_hidden=opts.receiver_hidden)
    loss = differentiable_loss(n_features, alphabet_size, max_len, alpha)

    return sender, receiver, loss


if __name__ == "__main__":
    opts = get_params()

    print(f'Launching game with parameters: {opts}')

    device = torch.device("cuda" if opts.cuda else "cpu")

    train_loader = None
    if opts.train_data:
        train_loader = DataLoader(CSVDataset(path=opts.train_data),
                                  batch_size=opts.batch_size,
                                  shuffle=True, num_workers=1)

    validation_loader = None
    if opts.validation_data:
        validation_loader = DataLoader(CSVDataset(path=opts.validation_data),
                                       batch_size=opts.batch_size,
                                       shuffle=False, num_workers=1)

    dump_loader = None
    if opts.dump_data:
        dump_loader = DataLoader(CSVDataset(path=opts.dump_data),
                                 batch_size=opts.batch_size,
                                 shuffle=False, num_workers=1)

    assert train_loader or dump_loader, 'Either training or dump data must be specified'
    sender, receiver, loss = build_model(opts, train_loader, dump_loader)

    sender = RnnSenderGS(sender, opts.vocab_size, opts.sender_embedding, opts.sender_hidden,
                         cell=opts.sender_cell, max_len=opts.max_len, temperature=opts.temperature, straight_through=opts.st)

    game = None
    if opts.bidirectional:
        receiver = BiRnnReceiverSTGS(receiver, opts.vocab_size, opts.receiver_embedding,
                                      opts.receiver_hidden, cell=opts.receiver_cell)
        game = SenderReceiverBiRnnSTGS(sender, receiver, loss)
    else:
        receiver = core.RnnReceiverGS(receiver, opts.vocab_size, opts.receiver_embedding,
                                      opts.receiver_hidden, cell=opts.receiver_cell)
        game = SenderReceiverRnnGS(sender, receiver, loss)

    optimizer = torch.optim.Adam([
        {'params': game.sender.parameters(), 'lr': opts.sender_lr},
        {'params': game.receiver.parameters(), 'lr': opts.receiver_lr}
    ])

    trainer = core.Trainer(game=game, optimizer=optimizer, train_data=train_loader,
                           validation_data=validation_loader, callbacks=[core.ConsoleLogger(print_train_loss=True, as_json=True), TopographicSimilarity(sender_input_distance_fn='cosine', message_distance_fn='edit', is_gumbel=True)])

    if dump_loader is not None:
        print("Printing vocab...")
        lines = []
        for vector, label in dump_loader:
            vector = torch.tensor(vector).cuda()
            with torch.no_grad():
                message, _ = sender(vector)
                message = message.argmax(dim=-1).cpu().numpy()[0]
                message = ' '.join([str(i) for i in message])
                label = label.item()
                lines.append("{label} {message}".format(label=label, message=message))
            with open(opts.output_strings, 'w') as f:
                f.write('\n'.join(lines))
    else:
        trainer.train(n_epochs=opts.n_epochs)

    core.close()

